"""
è®¢å•æ•°æ®å¤„ç†å™¨ - æ”¯æŒè‡ªå®šä¹‰æ—¥æœŸèŒƒå›´å’Œå®Œæ•´åŠŸèƒ½
backend/app/core/orders/orders_processor.py
"""

import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from .virtual_data_generator import VirtualOrderDataGenerator


class OrdersProcessor:
    """è®¢å•æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®¢å•å¤„ç†å™¨"""
        self.data = pd.DataFrame()
        self.filtered_data = pd.DataFrame()
        self.generator = VirtualOrderDataGenerator()
        self._last_generation_params = {}
    
    def _ensure_json_serializable(self, data: Any) -> Any:
        """ç¡®ä¿æ•°æ®æ˜¯JSONå¯åºåˆ—åŒ–çš„ï¼Œå¤„ç†numpyç±»å‹"""
        import numpy as np
        
        if isinstance(data, dict):
            return {k: self._ensure_json_serializable(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._ensure_json_serializable(item) for item in data]
        elif isinstance(data, (np.integer, np.int64, np.int32)):
            return int(data)
        elif isinstance(data, (np.floating, np.float64, np.float32)):
            return float(data)
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif pd.isna(data):
            return None
        else:
            return data
    
    def _parse_date_range(self, date_range_dict: Dict[str, str]) -> Tuple[datetime, datetime]:
        """
        è§£ææ—¥æœŸèŒƒå›´å­—å…¸ä¸ºdatetimeå¯¹è±¡
        
        Args:
            date_range_dict: åŒ…å«start_dateå’Œend_dateçš„å­—å…¸
            
        Returns:
            (start_date, end_date) datetimeå…ƒç»„
        """
        try:
            start_date_str = date_range_dict.get("start_date")
            end_date_str = date_range_dict.get("end_date")
            
            if not start_date_str or not end_date_str:
                raise ValueError("ç¼ºå°‘å¼€å§‹æ—¥æœŸæˆ–ç»“æŸæ—¥æœŸ")
            
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
            
            return start_date, end_date
        except ValueError as e:
            raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}")
    
    def generate_virtual_data(
        self, 
        count: int, 
        date_range: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè™šæ‹Ÿè®¢å•æ•°æ® - æ”¯æŒè‡ªå®šä¹‰æ—¥æœŸèŒƒå›´
        
        Args:
            count: è¦ç”Ÿæˆçš„è®¢å•æ•°é‡
            date_range: å¯é€‰çš„æ—¥æœŸèŒƒå›´å­—å…¸ {"start_date": "YYYY-MM-DD", "end_date": "YYYY-MM-DD"}
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            if count <= 0:
                return {
                    "success": False,
                    "message": "æ•°æ®æ¡æ•°å¿…é¡»å¤§äº0"
                }
            
            if count > 10000:
                return {
                    "success": False,
                    "message": "æ•°æ®æ¡æ•°ä¸èƒ½è¶…è¿‡10000"
                }
            
            # å¤„ç†æ—¥æœŸèŒƒå›´
            start_date = None
            end_date = None
            
            if date_range:
                try:
                    start_date, end_date = self._parse_date_range(date_range)
                    
                    # éªŒè¯æ—¥æœŸèŒƒå›´
                    validation = self.generator.validate_date_range(start_date, end_date)
                    if not validation["valid"]:
                        return {
                            "success": False,
                            "message": f"æ—¥æœŸèŒƒå›´éªŒè¯å¤±è´¥: {'; '.join(validation['errors'])}"
                        }
                except ValueError as e:
                    return {
                        "success": False,
                        "message": str(e)
                    }
            
            # ç”Ÿæˆæ•°æ®
            self.data = self.generator.generate_orders_batch(
                count=count,
                start_date=start_date,
                end_date=end_date
            )
            self.filtered_data = self.data.copy()
            
            # ä¿å­˜ç”Ÿæˆå‚æ•°
            self._last_generation_params = {
                "count": count,
                "date_range": date_range
            }
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.generator.get_generation_stats(self.data)
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            serializable_stats = self._ensure_json_serializable(stats)
            
            # æ„å»ºæˆåŠŸæ¶ˆæ¯
            message = f"æˆåŠŸç”Ÿæˆ {len(self.data)} æ¡è™šæ‹Ÿè®¢å•æ•°æ®"
            if date_range:
                message += f"ï¼Œæ—¶é—´èŒƒå›´ï¼š{date_range['start_date']} è‡³ {date_range['end_date']}"
            
            return {
                "success": True,
                "generated_count": len(self.data),
                "stats": serializable_stats,
                "message": message
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "ç”Ÿæˆè™šæ‹Ÿè®¢å•æ•°æ®å¤±è´¥"
            }
    
    def apply_filters(
        self,
        date_range: Optional[Tuple[str, str]] = None,
        order_types: Optional[List[str]] = None,
        license_ids: Optional[List[int]] = None,
        currencies: Optional[List[str]] = None,
        payment_platforms: Optional[List[str]] = None,
        order_statuses: Optional[List[str]] = None,
        sales_amount_range: Optional[Tuple[float, float]] = None,
        has_coupon: Optional[bool] = None,
        ab_test_filter: Optional[str] = None  # "with", "without", None
    ) -> Dict[str, Any]:
        """åº”ç”¨ç­›é€‰æ¡ä»¶"""
        try:
            if self.data.empty:
                return {
                    "success": False,
                    "message": "æ²¡æœ‰æ•°æ®å¯ä»¥ç­›é€‰ï¼Œè¯·å…ˆç”Ÿæˆè™šæ‹Ÿæ•°æ®"
                }
            
            filtered_df = self.data.copy()
            
            # æ—¥æœŸèŒƒå›´ç­›é€‰
            if date_range:
                start_date, end_date = date_range
                try:
                    filtered_df = filtered_df[
                        (pd.to_datetime(filtered_df["æ—¥æœŸ"]) >= pd.to_datetime(start_date)) &
                        (pd.to_datetime(filtered_df["æ—¥æœŸ"]) <= pd.to_datetime(end_date))
                    ]
                except Exception as e:
                    return {
                        "success": False,
                        "message": f"æ—¥æœŸèŒƒå›´ç­›é€‰å¤±è´¥: {str(e)}"
                    }
            
            # è®¢å•ç±»å‹ç­›é€‰
            if order_types:
                filtered_df = filtered_df[filtered_df["è®¢å•ç±»å‹"].isin(order_types)]
            
            # License IDç­›é€‰
            if license_ids:
                filtered_df = filtered_df[filtered_df["LicenseID"].isin(license_ids)]
            
            # å¸ç§ç­›é€‰
            if currencies:
                filtered_df = filtered_df[filtered_df["æ”¯ä»˜å¸ç§"].isin(currencies)]
            
            # æ”¯ä»˜å¹³å°ç­›é€‰
            if payment_platforms:
                filtered_df = filtered_df[filtered_df["æ”¯ä»˜å¹³å°"].isin(payment_platforms)]
            
            # è®¢å•çŠ¶æ€ç­›é€‰
            if order_statuses:
                filtered_df = filtered_df[filtered_df["è®¢å•çŠ¶æ€"].isin(order_statuses)]
            
            # é”€å”®æ€»é¢èŒƒå›´ç­›é€‰
            if sales_amount_range:
                min_amount, max_amount = sales_amount_range
                filtered_df = filtered_df[
                    (filtered_df["é”€å”®æ€»é¢"] >= min_amount) &
                    (filtered_df["é”€å”®æ€»é¢"] <= max_amount)
                ]
            
            # ä¼˜æƒ åˆ¸ç­›é€‰
            if has_coupon is not None:
                if has_coupon:
                    filtered_df = filtered_df[filtered_df["CouponID"].notna()]
                else:
                    filtered_df = filtered_df[filtered_df["CouponID"].isna()]
            
            # ABæµ‹è¯•ç­›é€‰
            if ab_test_filter == "with":
                filtered_df = filtered_df[filtered_df["ABå®éªŒID"].notna()]
            elif ab_test_filter == "without":
                filtered_df = filtered_df[filtered_df["ABå®éªŒID"].isna()]
            
            self.filtered_data = filtered_df
            
            # è®¡ç®—ç­›é€‰åçš„ç»Ÿè®¡ä¿¡æ¯
            filtered_stats = self.generator.get_generation_stats(self.filtered_data)
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            serializable_stats = self._ensure_json_serializable(filtered_stats)
            
            return {
                "success": True,
                "filtered_count": len(self.filtered_data),
                "filtered_stats": serializable_stats,
                "message": f"ç­›é€‰å®Œæˆï¼Œå…± {len(self.filtered_data)} æ¡è®°å½•"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "åº”ç”¨ç­›é€‰æ¡ä»¶å¤±è´¥"
            }
    
    def get_chart_data(self) -> Dict[str, Any]:
        """è·å–å›¾è¡¨æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ·»åŠ æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†"""
        
        # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦å°±ç»ª
        if self.filtered_data.empty:
            # å¦‚æœç­›é€‰æ•°æ®ä¸ºç©ºï¼Œä½†åŸå§‹æ•°æ®ä¸ä¸ºç©ºï¼Œé‡æ–°è®¾ç½®ç­›é€‰æ•°æ®
            if not self.data.empty:
                self.filtered_data = self.data.copy()
            else:
                return {
                    "charts": {},
                    "error": "æ²¡æœ‰å¯ç”¨çš„æ•°æ®ç”Ÿæˆå›¾è¡¨",
                    "debug_info": {
                        "original_data_empty": self.data.empty,
                        "filtered_data_empty": self.filtered_data.empty
                    }
                }
        
        try:
            df = self.filtered_data.copy()
            
            # ğŸ”§ æ–°å¢ï¼šæ•°æ®å®Œæ•´æ€§éªŒè¯
            required_columns = ["è®¢å•ç±»å‹", "æ—¥æœŸ", "LicenseID", "æ”¯ä»˜å¸ç§", "æ”¯ä»˜å¹³å°", "è®¢å•çŠ¶æ€", "é”€å”®æ€»é¢"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                return {
                    "charts": {},
                    "error": f"æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_columns)}",
                    "debug_info": {
                        "available_columns": list(df.columns),
                        "missing_columns": missing_columns
                    }
                }
            
            # éªŒè¯æ•°æ®ä¸ä¸ºç©º
            if len(df) == 0:
                return {
                    "charts": {},
                    "error": "ç­›é€‰åçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨",
                    "debug_info": {
                        "original_data_count": len(self.data),
                        "filtered_data_count": len(df)
                    }
                }
            
            # ğŸ”§ ä¼˜åŒ–ï¼šæ·»åŠ æ•°æ®ç±»å‹éªŒè¯å’Œè½¬æ¢
            try:
                # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"])
                
                # ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ­£ç¡®çš„ç±»å‹
                if "é”€å”®æ€»é¢" in df.columns:
                    df["é”€å”®æ€»é¢"] = pd.to_numeric(df["é”€å”®æ€»é¢"], errors='coerce')
                
                if "LicenseID" in df.columns:
                    df["LicenseID"] = pd.to_numeric(df["LicenseID"], errors='coerce')
                
            except Exception as type_error:
                return {
                    "charts": {},
                    "error": f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {str(type_error)}",
                    "debug_info": {
                        "data_types": {col: str(dtype) for col, dtype in df.dtypes.items()}
                    }
                }
            
            # 1. è®¢å•ç±»å‹åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰
            order_type_data = {str(k): int(v) for k, v in df["è®¢å•ç±»å‹"].value_counts().to_dict().items()}
            
            # 2. æ¯æ—¥è®¢å•é‡è¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰
            df["æ—¥æœŸ_date"] = df["æ—¥æœŸ"].dt.date
            daily_orders = df.groupby("æ—¥æœŸ_date").size().reset_index(name="è®¢å•æ•°é‡")
            daily_orders["æ—¥æœŸ"] = daily_orders["æ—¥æœŸ_date"].astype(str)
            # ç¡®ä¿æ•°æ®ç±»å‹è½¬æ¢
            daily_orders_data = []
            for _, row in daily_orders.iterrows():
                daily_orders_data.append({
                    "æ—¥æœŸ": str(row["æ—¥æœŸ"]),
                    "è®¢å•æ•°é‡": int(row["è®¢å•æ•°é‡"])
                })
            
            # 3. Licenseç±»å‹é”€å”®åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰
            license_stats = df.groupby("LicenseID").agg({
                "è®¢å•å·": "count",
                "é”€å”®æ€»é¢": "sum"
            }).reset_index()
            license_stats.columns = ["LicenseID", "è®¢å•æ•°é‡", "é”€å”®æ€»é¢"]
            # ç¡®ä¿æ•°æ®ç±»å‹è½¬æ¢
            license_stats_data = []
            for _, row in license_stats.iterrows():
                license_stats_data.append({
                    "LicenseID": int(row["LicenseID"]),
                    "è®¢å•æ•°é‡": int(row["è®¢å•æ•°é‡"]),
                    "é”€å”®æ€»é¢": float(row["é”€å”®æ€»é¢"])
                })
            
            # 4. å¸ç§æ”¶å…¥åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰
            currency_revenue = {str(k): float(v) for k, v in df.groupby("æ”¯ä»˜å¸ç§")["é”€å”®æ€»é¢"].sum().to_dict().items()}
            
            # 5. æ”¯ä»˜å¹³å°åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰
            platform_stats = df.groupby("æ”¯ä»˜å¹³å°").agg({
                "è®¢å•å·": "count",
                "é”€å”®æ€»é¢": "sum"
            }).reset_index()
            platform_stats.columns = ["æ”¯ä»˜å¹³å°", "è®¢å•æ•°é‡", "é”€å”®æ€»é¢"]
            # ç¡®ä¿æ•°æ®ç±»å‹è½¬æ¢
            platform_stats_data = []
            for _, row in platform_stats.iterrows():
                platform_stats_data.append({
                    "æ”¯ä»˜å¹³å°": str(row["æ”¯ä»˜å¹³å°"]),
                    "è®¢å•æ•°é‡": int(row["è®¢å•æ•°é‡"]),
                    "é”€å”®æ€»é¢": float(row["é”€å”®æ€»é¢"])
                })
            
            # 6. è®¢å•çŠ¶æ€åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰
            status_distribution = {str(k): int(v) for k, v in df["è®¢å•çŠ¶æ€"].value_counts().to_dict().items()}
            
            # 7. ä¼˜æƒ åˆ¸ä½¿ç”¨æƒ…å†µï¼ˆæŸ±çŠ¶å›¾ï¼‰
            coupon_usage_data = [
                {"category": "æœ‰ä¼˜æƒ åˆ¸", "count": int(len(df[df["CouponID"].notna()]))},
                {"category": "æ— ä¼˜æƒ åˆ¸", "count": int(len(df[df["CouponID"].isna()]))}
            ]
            
            # 8. ABæµ‹è¯•å‚ä¸æƒ…å†µ
            ab_test_participation = {
                "å‚ä¸ABæµ‹è¯•": int(len(df[df["ABå®éªŒID"].notna()])),
                "æœªå‚ä¸ABæµ‹è¯•": int(len(df[df["ABå®éªŒID"].isna()]))
            }
            
            # ğŸ”§ æ–°å¢ï¼šæ„å»ºå›¾è¡¨æ•°æ®ç»“æ„
            chart_data = {
                "charts": {
                    "order_type_distribution": {
                        "type": "pie",
                        "title": "è®¢å•ç±»å‹åˆ†å¸ƒ",
                        "data": order_type_data
                    },
                    "daily_orders_trend": {
                        "type": "line",
                        "title": "æ¯æ—¥è®¢å•é‡è¶‹åŠ¿",
                        "data": daily_orders_data
                    },
                    "license_sales_distribution": {
                        "type": "bar",
                        "title": "Licenseç±»å‹é”€å”®åˆ†å¸ƒ",
                        "data": license_stats_data
                    },
                    "currency_revenue_distribution": {
                        "type": "pie",
                        "title": "å¸ç§æ”¶å…¥åˆ†å¸ƒ",
                        "data": currency_revenue
                    },
                    "payment_platform_stats": {
                        "type": "bar",
                        "title": "æ”¯ä»˜å¹³å°ç»Ÿè®¡",
                        "data": platform_stats_data
                    },
                    "order_status_distribution": {
                        "type": "pie",
                        "title": "è®¢å•çŠ¶æ€åˆ†å¸ƒ",
                        "data": status_distribution
                    },
                    "coupon_usage": {
                        "type": "bar",
                        "title": "ä¼˜æƒ åˆ¸ä½¿ç”¨æƒ…å†µ",
                        "data": coupon_usage_data
                    },
                    "ab_test_participation": {
                        "type": "pie",
                        "title": "ABæµ‹è¯•å‚ä¸æƒ…å†µ",
                        "data": ab_test_participation
                    }
                },
                "metadata": {
                    "data_rows": len(df),
                    "generated_at": datetime.now().isoformat(),
                    "charts_count": 8,
                    "data_source": "filtered" if len(df) != len(self.data) else "original"
                }
            }
            
            # ğŸ”§ æ–°å¢ï¼šéªŒè¯æ‰€æœ‰å›¾è¡¨éƒ½æœ‰æ•°æ®
            empty_charts = []
            for chart_name, chart_config in chart_data["charts"].items():
                chart_data_content = chart_config.get("data")
                if not chart_data_content or (
                    isinstance(chart_data_content, (list, dict)) and len(chart_data_content) == 0
                ):
                    empty_charts.append(chart_name)
            
            if empty_charts:
                chart_data["warning"] = f"ä»¥ä¸‹å›¾è¡¨æ²¡æœ‰æ•°æ®: {', '.join(empty_charts)}"
            
            # ğŸ”§ æ–°å¢ï¼šæ·»åŠ æ•°æ®è´¨é‡ä¿¡æ¯
            chart_data["data_quality"] = {
                "has_all_required_fields": len(missing_columns) == 0,
                "data_completeness": {
                    "total_records": len(df),
                    "null_counts": {col: int(df[col].isna().sum()) for col in required_columns if col in df.columns}
                },
                "value_ranges": {
                    "sales_amount": {
                        "min": float(df["é”€å”®æ€»é¢"].min()) if "é”€å”®æ€»é¢" in df.columns else None,
                        "max": float(df["é”€å”®æ€»é¢"].max()) if "é”€å”®æ€»é¢" in df.columns else None,
                        "mean": float(df["é”€å”®æ€»é¢"].mean()) if "é”€å”®æ€»é¢" in df.columns else None
                    }
                }
            }
            
            return chart_data
            
        except Exception as e:
            return {
                "charts": {},
                "error": f"ç”Ÿæˆå›¾è¡¨æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "debug_info": {
                    "data_shape": list(self.filtered_data.shape) if not self.filtered_data.empty else "empty",
                    "columns": list(self.filtered_data.columns) if not self.filtered_data.empty else [],
                    "error_type": type(e).__name__,
                    "error_details": str(e)
                }
            }
    
    def export_filtered_data(self) -> bytes:
        """å¯¼å‡ºç­›é€‰åçš„æ•°æ®ä¸ºCSV"""
        if self.filtered_data.empty:
            return b"No data to export"
        
        try:
            # ä½¿ç”¨UTF-8ç¼–ç å¹¶æ·»åŠ BOMä»¥ç¡®ä¿Excelæ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
            csv_bytes = self.filtered_data.to_csv(index=False, encoding='utf-8-sig').encode('utf-8')
            return csv_bytes
        except Exception as e:
            error_msg = f"å¯¼å‡ºå¤±è´¥: {str(e)}"
            return error_msg.encode('utf-8')
    
    def get_filter_ranges(self) -> Dict[str, Any]:
        """è·å–ç­›é€‰èŒƒå›´"""
        if self.data.empty:
            return {
                "date_range": {
                    "min": "2025-04-01",
                    "max": "2025-05-31"
                },
                "sales_amount_range": {
                    "min": 0,
                    "max": 100
                },
                "available_options": {
                    "order_types": ["æ–°å•", "ç»­è´¹"],
                    "license_ids": [1, 2, 3, 4, 5],
                    "currencies": ["usd", "cny", "eur"],
                    "payment_platforms": ["paypal", "stripe"],
                    "order_statuses": ["å·²ä»˜æ¬¾", "å·²é€€æ¬¾", "å–æ¶ˆä»˜æ¬¾", "ä»˜æ¬¾å¤±è´¥"]
                }
            }
        
        try:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
            unique_order_types = [str(x) for x in self.data["è®¢å•ç±»å‹"].unique().tolist()]
            unique_license_ids = [int(x) for x in self.data["LicenseID"].unique().tolist()]
            unique_currencies = [str(x) for x in self.data["æ”¯ä»˜å¸ç§"].unique().tolist()]
            unique_payment_platforms = [str(x) for x in self.data["æ”¯ä»˜å¹³å°"].unique().tolist()]
            unique_order_statuses = [str(x) for x in self.data["è®¢å•çŠ¶æ€"].unique().tolist()]
            
            return {
                "date_range": {
                    "min": str(self.data["æ—¥æœŸ"].min()),
                    "max": str(self.data["æ—¥æœŸ"].max())
                },
                "sales_amount_range": {
                    "min": float(self.data["é”€å”®æ€»é¢"].min()),
                    "max": float(self.data["é”€å”®æ€»é¢"].max())
                },
                "available_options": {
                    "order_types": sorted(unique_order_types),
                    "license_ids": sorted(unique_license_ids),
                    "currencies": sorted(unique_currencies),
                    "payment_platforms": sorted(unique_payment_platforms),
                    "order_statuses": sorted(unique_order_statuses)
                }
            }
        except Exception as e:
            return {
                "error": str(e),
                "date_range": {"min": "", "max": ""},
                "sales_amount_range": {"min": 0, "max": 100},
                "available_options": {
                    "order_types": [],
                    "license_ids": [],
                    "currencies": [],
                    "payment_platforms": [],
                    "order_statuses": []
                }
            }
    
    def get_data_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        return {
            "total_orders": int(len(self.data)) if not self.data.empty else 0,
            "filtered_orders": int(len(self.filtered_data)) if not self.filtered_data.empty else 0,
            "has_data": not self.data.empty,
            "last_generation_params": self._last_generation_params
        }
    
    def reset_data(self) -> None:
        """é‡ç½®æ‰€æœ‰æ•°æ®"""
        self.data = pd.DataFrame()
        self.filtered_data = pd.DataFrame()
        self._last_generation_params = {}
    
    def get_detailed_statistics(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰çš„é¢å¤–åŠŸèƒ½ï¼‰"""
        if self.filtered_data.empty:
            return {}
        
        try:
            df = self.filtered_data
            
            # åŸºç¡€ç»Ÿè®¡
            basic_stats = {
                "total_orders": int(len(df)),
                "unique_users": int(df["ç”¨æˆ·ID"].nunique()),
                "date_range": {
                    "start": str(df["æ—¥æœŸ"].min()),
                    "end": str(df["æ—¥æœŸ"].max())
                }
            }
            
            # æ”¶å…¥ç»Ÿè®¡
            revenue_stats = {}
            for currency in df["æ”¯ä»˜å¸ç§"].unique():
                currency_df = df[df["æ”¯ä»˜å¸ç§"] == currency]
                revenue_stats[str(currency)] = {
                    "total_revenue": float(currency_df["é”€å”®æ€»é¢"].sum()),
                    "avg_order_value": float(currency_df["é”€å”®æ€»é¢"].mean()),
                    "order_count": int(len(currency_df)),
                    "max_order": float(currency_df["é”€å”®æ€»é¢"].max()),
                    "min_order": float(currency_df["é”€å”®æ€»é¢"].min())
                }
            
            # Licenseç»Ÿè®¡
            license_stats = {}
            for license_id in df["LicenseID"].unique():
                license_df = df[df["LicenseID"] == license_id]
                license_stats[int(license_id)] = {
                    "order_count": int(len(license_df)),
                    "total_revenue": float(license_df["é”€å”®æ€»é¢"].sum()),
                    "avg_revenue": float(license_df["é”€å”®æ€»é¢"].mean())
                }
            
            # æ—¶é—´åˆ†æ
            df_time = df.copy()
            df_time["æ—¥æœŸ_datetime"] = pd.to_datetime(df_time["æ—¥æœŸ"])
            df_time["hour"] = df_time["æ—¥æœŸ_datetime"].dt.hour
            df_time["weekday"] = df_time["æ—¥æœŸ_datetime"].dt.dayofweek
            
            time_stats = {
                "hourly_distribution": {str(k): int(v) for k, v in df_time["hour"].value_counts().sort_index().to_dict().items()},
                "weekday_distribution": {str(k): int(v) for k, v in df_time["weekday"].value_counts().sort_index().to_dict().items()},
                "peak_hour": int(df_time["hour"].mode().iloc[0]) if not df_time["hour"].mode().empty else 0,
                "peak_weekday": int(df_time["weekday"].mode().iloc[0]) if not df_time["weekday"].mode().empty else 0
            }
            
            # ä¼˜æƒ åˆ¸æ•ˆæœåˆ†æ
            coupon_stats = {
                "coupon_usage_rate": float(len(df[df["CouponID"].notna()]) / len(df) * 100),
                "avg_discount_amount": 0.0,
                "coupon_revenue_impact": 0.0
            }
            
            # ABæµ‹è¯•åˆ†æ
            ab_test_stats = {
                "participation_rate": float(len(df[df["ABå®éªŒID"].notna()]) / len(df) * 100),
                "ab_test_groups": {str(k): int(v) for k, v in df["ABå®éªŒID"].value_counts().to_dict().items()}
            }
            
            return {
                "basic": basic_stats,
                "revenue": revenue_stats,
                "license": license_stats,
                "time": time_stats,
                "coupon": coupon_stats,
                "ab_test": ab_test_stats
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆå¯é€‰çš„è´¨é‡æ£€æŸ¥åŠŸèƒ½ï¼‰"""
        if self.data.empty:
            return {"is_valid": True, "issues": []}
        
        issues = []
        
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = [
                "è®¢å•å·", "è®¢å•ç±»å‹", "ç”¨æˆ·ID", "æ—¥æœŸ", "äº§å“ï¼ˆPIDï¼‰", 
                "LicenseID", "SKU", "è´­ç‰©è½¦æ¥æº", "æ”¯ä»˜å¹³å°", "æ”¯ä»˜å¸ç§", 
                "æ”¯ä»˜æ–¹å¼", "é”€å”®æ€»é¢", "è®¢å•çŠ¶æ€"
            ]
            
            for field in required_fields:
                if field not in self.data.columns:
                    issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                elif self.data[field].isna().any():
                    null_count = int(self.data[field].isna().sum())
                    issues.append(f"å­—æ®µ {field} æœ‰ {null_count} ä¸ªç©ºå€¼")
            
            # æ£€æŸ¥è®¢å•å·å”¯ä¸€æ€§
            if "è®¢å•å·" in self.data.columns:
                duplicate_orders = int(self.data["è®¢å•å·"].duplicated().sum())
                if duplicate_orders > 0:
                    issues.append(f"å‘ç° {duplicate_orders} ä¸ªé‡å¤è®¢å•å·")
            
            # æ£€æŸ¥æ—¥æœŸæ ¼å¼
            if "æ—¥æœŸ" in self.data.columns:
                try:
                    pd.to_datetime(self.data["æ—¥æœŸ"])
                except:
                    issues.append("æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®")
            
            # æ£€æŸ¥é”€å”®æ€»é¢
            if "é”€å”®æ€»é¢" in self.data.columns:
                negative_amounts = int((self.data["é”€å”®æ€»é¢"] < 0).sum())
                if negative_amounts > 0:
                    issues.append(f"å‘ç° {negative_amounts} ä¸ªè´Ÿçš„é”€å”®æ€»é¢")
            
            # æ£€æŸ¥SKUæ ¼å¼
            if "SKU" in self.data.columns and "äº§å“ï¼ˆPIDï¼‰" in self.data.columns and "LicenseID" in self.data.columns:
                invalid_sku = 0
                for _, row in self.data.iterrows():
                    expected_sku = f"{row['äº§å“ï¼ˆPIDï¼‰']}{row['LicenseID']:03d}"
                    if str(row["SKU"]) != expected_sku:
                        invalid_sku += 1
                
                if invalid_sku > 0:
                    issues.append(f"å‘ç° {invalid_sku} ä¸ªæ— æ•ˆçš„SKUæ ¼å¼")
            
            return {
                "is_valid": len(issues) == 0,
                "issues": issues,
                "total_records": int(len(self.data)),
                "validation_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "is_valid": False,
                "issues": [f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"],
                "total_records": int(len(self.data)),
                "validation_time": datetime.now().isoformat()
            }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        try:
            import psutil
            import os
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            # æ•°æ®å¤§å°ä¼°ç®—
            data_size = 0
            filtered_data_size = 0
            
            if not self.data.empty:
                data_size = self.data.memory_usage(deep=True).sum()
            
            if not self.filtered_data.empty:
                filtered_data_size = self.filtered_data.memory_usage(deep=True).sum()
            
            return {
                "memory": {
                    "rss": int(memory_info.rss),  # ç‰©ç†å†…å­˜ä½¿ç”¨
                    "vms": int(memory_info.vms),  # è™šæ‹Ÿå†…å­˜ä½¿ç”¨
                    "data_memory": int(data_size),  # æ•°æ®å ç”¨å†…å­˜
                    "filtered_data_memory": int(filtered_data_size)  # ç­›é€‰åæ•°æ®å ç”¨å†…å­˜
                },
                "data": {
                    "total_rows": int(len(self.data)) if not self.data.empty else 0,
                    "filtered_rows": int(len(self.filtered_data)) if not self.filtered_data.empty else 0,
                    "columns": len(self.data.columns) if not self.data.empty else 0,
                    "data_types": {col: str(dtype) for col, dtype in self.data.dtypes.items()} if not self.data.empty else {}
                },
                "generation_params": self._last_generation_params
            }
        except ImportError:
            return {
                "error": "psutil not available",
                "data": {
                    "total_rows": int(len(self.data)) if not self.data.empty else 0,
                    "filtered_rows": int(len(self.filtered_data)) if not self.filtered_data.empty else 0,
                    "columns": len(self.data.columns) if not self.data.empty else 0
                }
            }
        except Exception as e:
            return {"error": str(e)}
    
    def export_data_sample(self, sample_size: int = 100) -> Dict[str, Any]:
        """å¯¼å‡ºæ•°æ®æ ·æœ¬ç”¨äºé¢„è§ˆï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        try:
            if self.filtered_data.empty:
                return {
                    "success": False,
                    "message": "æ²¡æœ‰æ•°æ®å¯ä»¥å¯¼å‡ºæ ·æœ¬"
                }
            
            # è·å–æ ·æœ¬æ•°æ®
            sample_df = self.filtered_data.head(sample_size)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            sample_data = []
            for _, row in sample_df.iterrows():
                row_dict = {}
                for col in sample_df.columns:
                    value = row[col]
                    # ç¡®ä¿å€¼æ˜¯JSONå¯åºåˆ—åŒ–çš„
                    if pd.isna(value):
                        row_dict[col] = None
                    else:
                        row_dict[col] = self._ensure_json_serializable(value)
                sample_data.append(row_dict)
            
            return {
                "success": True,
                "sample_size": len(sample_data),
                "total_size": len(self.filtered_data),
                "columns": list(sample_df.columns),
                "data": sample_data
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "å¯¼å‡ºæ•°æ®æ ·æœ¬å¤±è´¥"
            }